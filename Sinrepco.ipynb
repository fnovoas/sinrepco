{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhHIUuQEnBu+/anCkcALk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fnovoas/sinrepco/blob/main/Sinrepco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SINREPCO\n",
        "por fnovoas@unal.edu.co"
      ],
      "metadata": {
        "id": "L3tPnZ558pQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Instalamos las bibliotecas necesarias: TensorFlow para el desarrollo de modelos de inteligencia artificial, OpenCV para el procesamiento de imágenes y Tesseract para el reconocimiento óptico de caracteres (OCR)."
      ],
      "metadata": {
        "id": "VSwHf7ak8b99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQvguu1Q6a7a",
        "outputId": "225ffd96-9e4f-41b2-c7f7-a31a04841637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Procedemos con la recolección de datos, reunimos un conjunto de datos que contiene imágenes de vehículos, tanto contaminantes como no contaminantes, estas imágenes fueron capturadas por mí en diversas ubicaciones. Contamos con 3381 imágenes de vehículos no chimenea y 280 imágenes de vehículos chimenea.\n",
        "Separamos las imágenes según si contienen vehículos que emiten humo visible o no, en dos carpetas: \"chimenea\" y \"no_chimenea\". Con estos datos entrenaremos al modelo.\\\n",
        "Montamos Drive para acceder a las imágenes.\n"
      ],
      "metadata": {
        "id": "mkPe4Udv9xZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Fs2QnY1LnLWe",
        "outputId": "dbf96a16-46c2-44c4-84e5-9661be6afcb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "d3OhO9RXnaAR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cargamos y preparamos los datos de la carpeta chimenea. Este código divide las imágenes en conjuntos de entrenamiento, validación y prueba según el porcentaje especificado."
      ],
      "metadata": {
        "id": "4fB5L0_InkqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "'''\n",
        "#listar los elementos de la carpeta, para comprobar que accedió\n",
        "import os\n",
        "print(os.listdir(base_dir))\n",
        "'''\n",
        "\n",
        "def split_data(SOURCE, TRAINING, VALIDATION, TEST, SPLIT_SIZE):\n",
        "    all_files = os.listdir(SOURCE)\n",
        "    random.shuffle(all_files)\n",
        "    split_train = int(len(all_files) * SPLIT_SIZE)\n",
        "    split_val = int(len(all_files) * (1 - SPLIT_SIZE) / 2)\n",
        "\n",
        "    train_files = all_files[:split_train]\n",
        "    val_files = all_files[split_train:split_train + split_val]\n",
        "    test_files = all_files[split_train + split_val:]\n",
        "\n",
        "    for file_name in train_files:\n",
        "        source_file = os.path.join(SOURCE, file_name)\n",
        "        dest_file = os.path.join(TRAINING, file_name)\n",
        "        shutil.copyfile(source_file, dest_file)\n",
        "\n",
        "    for file_name in val_files:\n",
        "        source_file = os.path.join(SOURCE, file_name)\n",
        "        dest_file = os.path.join(VALIDATION, file_name)\n",
        "        shutil.copyfile(source_file, dest_file)\n",
        "\n",
        "    for file_name in test_files:\n",
        "        source_file = os.path.join(SOURCE, file_name)\n",
        "        dest_file = os.path.join(TEST, file_name)\n",
        "        shutil.copyfile(source_file, dest_file)\n",
        "\n",
        "# Configurar las rutas\n",
        "base_dir = '/content/drive/My Drive/sinrepco_fotos'\n",
        "source_chimenea = f'{base_dir}/chimenea'\n",
        "source_no_chimenea = f'{base_dir}/no_chimenea'\n",
        "\n",
        "train_chimenea_dir = f'{base_dir}/train/chimenea'\n",
        "val_chimenea_dir = f'{base_dir}/validation/chimenea'\n",
        "test_chimenea_dir = f'{base_dir}/test/chimenea'\n",
        "\n",
        "train_no_chimenea_dir = f'{base_dir}/train/no_chimenea'\n",
        "val_no_chimenea_dir = f'{base_dir}/validation/no_chimenea'\n",
        "test_no_chimenea_dir = f'{base_dir}/test/no_chimenea'\n",
        "\n",
        "os.makedirs(train_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(val_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(test_chimenea_dir, exist_ok=True)\n",
        "\n",
        "os.makedirs(train_no_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(val_no_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(test_no_chimenea_dir, exist_ok=True)\n",
        "\n",
        "# Dividir los datos\n",
        "split_size = 0.7\n",
        "split_data(source_chimenea, train_chimenea_dir, val_chimenea_dir, test_chimenea_dir, split_size)\n",
        "split_data(source_no_chimenea, train_no_chimenea_dir, val_no_chimenea_dir, test_no_chimenea_dir, split_size)\n"
      ],
      "metadata": {
        "id": "twbvWtshnngg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de dividir los datos, configuramos el ImageDataGenerator para cargar las imágenes de estas nuevas carpetas:"
      ],
      "metadata": {
        "id": "8yMhhElw4is4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros\n",
        "IMG_HEIGHT = 480\n",
        "IMG_WIDTH = 640\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{base_dir}/train',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    f'{base_dir}/validation',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    f'{base_dir}/test',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "TcdEMJal4j_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}