{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU2NgjkSy81B+OD6JOh75U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fnovoas/sinrepco/blob/main/Sinrepco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SINREPCO\n",
        "por fnovoas@unal.edu.co"
      ],
      "metadata": {
        "id": "L3tPnZ558pQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Instalamos las bibliotecas necesarias: TensorFlow para el desarrollo de modelos de inteligencia artificial, OpenCV para el procesamiento de imágenes y Tesseract para el reconocimiento óptico de caracteres (OCR)."
      ],
      "metadata": {
        "id": "VSwHf7ak8b99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQvguu1Q6a7a",
        "outputId": "6f64437c-2f4d-409a-91de-adbef93371ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Procedemos con la recolección de datos, reunimos un conjunto de datos que contiene imágenes de vehículos, tanto contaminantes como no contaminantes, estas imágenes fueron capturadas por mí en diversas ubicaciones. Contamos con 3381 imágenes de vehículos no chimenea y 280 imágenes de vehículos chimenea.\n",
        "Separamos las imágenes según si contienen vehículos que emiten humo visible o no, en dos carpetas: \"chimenea\" y \"no_chimenea\". Con estos datos entrenaremos al modelo.\\\n",
        "Montamos Drive para acceder a las imágenes.\n"
      ],
      "metadata": {
        "id": "mkPe4Udv9xZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montar el Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directorio base donde están las carpetas \"chimenea\" y \"no_chimenea\"\n",
        "base_dir = \"/content/drive/My Drive/sinrepco_fotos\"\n",
        "print(\"Carpetas encontradas: \")\n",
        "!ls \"/content/drive/My Drive/sinrepco_fotos\"\n",
        "# Rutas de las carpetas\n",
        "chimenea_dir = os.path.join(base_dir, \"chimenea\")\n",
        "no_chimenea_dir = os.path.join(base_dir, \"no_chimenea\")\n",
        "\n",
        "# Contar archivos en cada carpeta\n",
        "num_chimenea = len([f for f in os.listdir(chimenea_dir) if os.path.isfile(os.path.join(chimenea_dir, f))])\n",
        "num_no_chimenea = len([f for f in os.listdir(no_chimenea_dir) if os.path.isfile(os.path.join(no_chimenea_dir, f))])\n",
        "\n",
        "# Imprimir la cantidad de archivos\n",
        "print(f\"Número de archivos en la carpeta 'chimenea': {num_chimenea}\")\n",
        "print(f\"Número de archivos en la carpeta 'no_chimenea': {num_no_chimenea}\")"
      ],
      "metadata": {
        "id": "Fs2QnY1LnLWe",
        "outputId": "6b721a61-9a6f-4f45-890f-d01dff6521d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Carpetas encontradas: \n",
            "chimenea  no_chimenea\n",
            "Número de archivos en la carpeta 'chimenea': 280\n",
            "Número de archivos en la carpeta 'no_chimenea': 3381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "d3OhO9RXnaAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cargamos y preparamos los datos de la carpeta en donde están almacenados en Drive. Este código divide las imágenes en conjuntos de entrenamiento, validación y prueba según el porcentaje especificado: el 70% de los datos irán en entrenamiento y el 15% en cada una de las otras dos categorías. Ejecutar este código cambia la estructura de la organización de los archivos y carpetas en Drive. Solo hace falta ejecutar esta celda cuando incorporemos nuevos datos, al hacerlo, debemos tener todos los datos solamente en las dos carpetas iniciales."
      ],
      "metadata": {
        "id": "4fB5L0_InkqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "# Crear directorios si no existen\n",
        "base_dir = '/content/drive/My Drive/sinrepco_fotos'\n",
        "source_chimenea = f'{base_dir}/chimenea'\n",
        "source_no_chimenea = f'{base_dir}/no_chimenea'\n",
        "\n",
        "train_chimenea_dir = f'{base_dir}/train/chimenea'\n",
        "val_chimenea_dir = f'{base_dir}/validation/chimenea'\n",
        "test_chimenea_dir = f'{base_dir}/test/chimenea'\n",
        "\n",
        "train_no_chimenea_dir = f'{base_dir}/train/no_chimenea'\n",
        "val_no_chimenea_dir = f'{base_dir}/validation/no_chimenea'\n",
        "test_no_chimenea_dir = f'{base_dir}/test/no_chimenea'\n",
        "\n",
        "os.makedirs(train_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(val_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(test_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(train_no_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(val_no_chimenea_dir, exist_ok=True)\n",
        "os.makedirs(test_no_chimenea_dir, exist_ok=True)\n",
        "\n",
        "def move_files(files, source, destination):\n",
        "    for f in files:\n",
        "        src_path = os.path.join(source, f)\n",
        "        dest_path = os.path.join(destination, f)\n",
        "        shutil.move(src_path, dest_path)\n",
        "        # Verifica si el archivo se movió correctamente\n",
        "        if not os.path.exists(dest_path):\n",
        "            print(f\"Reintentando mover: {f}\")\n",
        "            shutil.move(src_path, dest_path)\n",
        "            # Da tiempo al sistema para procesar la operación\n",
        "            time.sleep(0.1)\n",
        "\n",
        "def move_data(SOURCE, TRAINING, VALIDATION, TEST, split_train=0.7, split_val_test=0.15):\n",
        "    files = [f for f in os.listdir(SOURCE) if os.path.isfile(os.path.join(SOURCE, f))]\n",
        "    print(f\"Total files found in {SOURCE}: {len(files)}\")  # Imprime el número total de archivos encontrados\n",
        "\n",
        "    random.shuffle(files)\n",
        "\n",
        "    train_size = int(len(files) * split_train)\n",
        "    val_size = int(len(files) * split_val_test)\n",
        "\n",
        "    train_files = files[:train_size]\n",
        "    val_files = files[train_size:train_size + val_size]\n",
        "    test_files = files[train_size + val_size:]\n",
        "\n",
        "    print(f\"Moving {len(train_files)} to {TRAINING}\")  # Imprime cuántos archivos se moverán al entrenamiento\n",
        "    print(f\"Moving {len(val_files)} to {VALIDATION}\")  # Imprime cuántos archivos se moverán a validación\n",
        "    print(f\"Moving {len(test_files)} to {TEST}\")  # Imprime cuántos archivos se moverán a prueba\n",
        "\n",
        "    # Mueve los archivos en lotes para evitar problemas con demasiadas operaciones a la vez\n",
        "    batch_size = 100  # Ajusta este tamaño según sea necesario\n",
        "    for i in range(0, len(train_files), batch_size):\n",
        "        move_files(train_files[i:i+batch_size], SOURCE, TRAINING)\n",
        "\n",
        "    for i in range(0, len(val_files), batch_size):\n",
        "        move_files(val_files[i:i+batch_size], SOURCE, VALIDATION)\n",
        "\n",
        "    for i in range(0, len(test_files), batch_size):\n",
        "        move_files(test_files[i:i+batch_size], SOURCE, TEST)\n",
        "\n",
        "# Ejecutar esta función para cada clase\n",
        "move_data(source_chimenea, train_chimenea_dir, val_chimenea_dir, test_chimenea_dir)\n",
        "move_data(source_no_chimenea, train_no_chimenea_dir, val_no_chimenea_dir, test_no_chimenea_dir)"
      ],
      "metadata": {
        "id": "twbvWtshnngg",
        "outputId": "4c58f6b1-c6e4-4d80-fe41-30cc1fa9d511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files found in /content/drive/My Drive/sinrepco_fotos/chimenea: 280\n",
            "Moving 196 to /content/drive/My Drive/sinrepco_fotos/train/chimenea\n",
            "Moving 42 to /content/drive/My Drive/sinrepco_fotos/validation/chimenea\n",
            "Moving 42 to /content/drive/My Drive/sinrepco_fotos/test/chimenea\n",
            "Total files found in /content/drive/My Drive/sinrepco_fotos/no_chimenea: 3381\n",
            "Moving 2366 to /content/drive/My Drive/sinrepco_fotos/train/no_chimenea\n",
            "Moving 507 to /content/drive/My Drive/sinrepco_fotos/validation/no_chimenea\n",
            "Moving 508 to /content/drive/My Drive/sinrepco_fotos/test/no_chimenea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de dividir los datos, configuramos el ImageDataGenerator para cargar las imágenes de estas nuevas carpetas:"
      ],
      "metadata": {
        "id": "8yMhhElw4is4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Import the ImageDataGenerator class\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "# Parámetros\n",
        "IMG_HEIGHT = 480\n",
        "IMG_WIDTH = 640\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{base_dir}/train',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    f'{base_dir}/validation',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    f'{base_dir}/test',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "TcdEMJal4j_O",
        "outputId": "aa0085d4-f60b-4743-f3bf-bab3841a1926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2562 images belonging to 2 classes.\n",
            "Found 549 images belonging to 2 classes.\n",
            "Found 550 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Definimos la arquitectura de la CNN. Tomamos una estructura de red neuronal convolucional, que es adecuada para el análisis de imágenes."
      ],
      "metadata": {
        "id": "DsbjWUhxNg3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(480, 640, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1jK4njrJNjfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}